---
title: "TWLogger Processing Scripts"
authors: "James Fahlbusch & Katie Harrington"
date: "August 18, 2019"
output: html_document
---

This processes one logger at a time. Must run entire script for each logger. Before beginning script, download deployment files into folder titled YYYYMMDD_[deploymentname] (e.g. 20190814_H38). 

Setup environment
```{r setup,echo=FALSE,results="hide",warning=FALSE,collapse=TRUE,include=FALSE}
# Set the Timezone Offset from UTC 
tzOffset <- "Etc/GMT+3" # Falklands time in the winter
# Set the Working Directory to the location of this File
current_path = rstudioapi::getActiveDocumentContext()$path 
setwd(dirname(current_path ))
rm(current_path)
getwd()
# This function loads required packages and installs them if they are not found
pkgTest <- function(x)
{
  if (!require(x,character.only = TRUE))
  {
    install.packages(x,dep=TRUE)
    if(!require(x,character.only = TRUE)) stop("Package not found")
  }
}
# Load Packages
pkgTest("tidyverse")
pkgTest("ggplot2")
pkgTest("lubridate")
pkgTest("leaflet")
pkgTest("htmlwidgets")
pkgTest("argosfilter")
pkgTest("zoo")
pkgTest("dplyr")
pkgTest("tagtools")
pkgTest("plotly")
pkgTest("RcppRoll")
pkgTest("car")
pkgTest("gridExtra")
pkgTest("signal")
pkgTest("ggpubr")
pkgTest("ggmap")
pkgTest("maps")
pkgTest("mapdata")
```

Define release and retrieval time for deployment
```{r}
# Reference 03_Subset to 24 hrs to improve code. Rerun 24-hr subset to extract start and end times of files and add to depMeta. Then call in depMeta here to define start and end times
# Specify the start and end time of deployment (NOTE: will be specific to each deployment, use local deployment time)
startTime <- as.POSIXct(strptime("2017-02-12 11:30:00",format="%Y-%m-%d %H:%M:%S"),tz=tzOffset)
# Always use logger retrieval time (regardless if retrieved before battery died)
endTime <- as.POSIXct(strptime("2017-02-13 16:37:00",format="%Y-%m-%d %H:%M:%S"),tz=tzOffset)
```

Step 1a: Combine raw CSV files 
```{r}
# Choose the folder containing the multiple CSV files
# Select the first CSV of the group you want to combine
filename <- file.choose("Select the first CSV of the group")
pathChoice <- dirname(filename)

# This imports all CSV files in the directory chosen
filenames <- list.files(path = pathChoice, pattern = "*.csv", all.files = FALSE, full.names = FALSE, recursive = FALSE, ignore.case = TRUE)
accdata <- do.call("rbind", sapply(paste(pathChoice,"/",filenames,sep = ""), read.csv, simplify = FALSE))


# Create a name column containing the date and tag number
accdata$name <- row.names(accdata)
deploymentName <- strsplit(accdata$name[1],'/')
deploymentName <- deploymentName[[1]][(length(deploymentName[[1]]))-1] #pulls name of deployment from row names (n-1 element of split string)
accdata$name <- deploymentName

# attr(data$dttz, "tzone") <- tzOffset # Use this to change from UTC to proper local time (time will change)
# data$dttz <- force_tz(data$dttz,tzone=tzOffset) # If time is correct and tz is wrong, force the tz (time will NOT change)

# Create proper datetime objects (convert GMT to local time zone of deployment) and extract GPS data
colnames(accdata)[1:2] <- c("date","time")
depYear <- year(as.POSIXct(paste(accdata$date[1], accdata$time[1]), format="%m/%d/%Y %H:%M:%S", tz="Etc/GMT+5")) # tz unimportant; only taking year
if(depYear==2017){
  # Add 2017 adjustment here
  accdata$dt <- as.POSIXct(paste(accdata$date, accdata$time), format="%m/%d/%Y %H:%M:%S", tz="Etc/GMT+5")
  attr(accdata$dt, "tzone") # Check tz
  attr(accdata$dt,"tzone") <- 'GMT'  # time will change
  accdata$dttz <- accdata$dt
  attr(accdata$dttz, "tzone") <- tzOffset  # time will change
  # Reorder columns to prepare to change column names
  accdata <- accdata[,c("name","dt","dttz","Timestamp.Ms.",
                      "ACCELX","ACCELY","ACCELZ","MAGX","MAGY","MAGZ")]
  # Change column names to more practical shorter names
  colnames(accdata)[1:10] <- c("name","dt","dttz","ts","ax","ay","az","mx","my","mz")
}else if(depYear==2018){
  # Add 2018 adjustment here
  # KJH: confirm these time zones adjustments
  accdata$dt <- as.POSIXct(paste(accdata$date, accdata$time), format="%m/%d/%Y %H:%M:%S", tz='Etc/GMT+7') # KJH: Double check this
  attr(accdata$dt,"tzone") # Check tz
  attr(accdata$dt,"tzone") <- 'GMT' # time will change
  accdata$dttz <- accdata$dt
  attr(accdata$dttz, "tzone") <- tzOffset  # time will change
  # Reorder columns to prepare to change column names
  accdata <- accdata[,c("name", "dt","dttz","Timestamp.Ms.",
                      "Temp.Raw.","ACCELX","ACCELY","ACCELZ","MAGX","MAGY","MAGZ","Sats",
                      "HDOP","Latitude","Longitude","FixAge","DateUTC","TimeUTC","DateAge","Altitude","Course","Speed")]
  # Change column names to more practical shorter names
  colnames(accdata)[1:22] <- c("name","dt","dttz","ts","temp","ax","ay","az","mx","my","mz",
                               "Sats","HDOP","Lat","Long","FixAge","DateUTC","TimeUTC","DateAge","Altitude","Course","Speed")
}else{
  # Add 2019 adjustment here 
  # KJH: confirm these time zones adjustments
  accdata$dt <- as.POSIXct(paste(accdata$date, accdata$time), format="%m/%d/%Y %H:%M:%S", tz='GMT') # KJH: Double check this
  attr(accdata$dt,"tzone") # Check tz
  attr(accdata$dt,"tzone") <- 'GMT' # time will change
  accdata$dttz <- accdata$dt
  attr(accdata$dttz, "tzone") <- tzOffset
  # Reorder columns to prepare to change column names
  accdata <- accdata[,c("name", "dt","dttz","Timestamp.Ms.",
                      "Temp.Raw.","ACCELX","ACCELY","ACCELZ","MAGX","MAGY","MAGZ","Sats",
                      "HDOP","Latitude","Longitude","FixAge","DateUTC","TimeUTC","DateAge","Altitude","Course","Speed")]
  # Change column names to more practical shorter names
  colnames(accdata)[1:22] <- c("name","dt","dttz","ts","temp","ax","ay","az","mx","my","mz",
                               "Sats","HDOP","Lat","Long","FixAge","DateUTC","TimeUTC","DateAge","Altitude","Course","Speed")
}

attr(accdata$dttz, "tzone") # check that dttz is in local time
str(accdata)
rownames(accdata) <- c()

# KJH: Fix this shit by pulling in depMeta
# Run subset() function to extract data for the selected timerange
# accdata <- subset(accdata, accdata$dttz >= startTime & accdata$dttz <= endTime)
summary(accdata) 

rm(pathChoice)
rm(filenames)

# Simple plot (plots a portion of the ACC data)
plotLength <- ifelse(length(accdata$ax)< 10000,length(accdata$ax),10000)
ggplot(data = accdata[1:plotLength,]) +
  geom_line(aes(x = dttz, y = ax,color = 'AX')) +
  geom_line(aes(x = dttz, y = ay,color = 'AY')) +
  geom_line(aes(x = dttz, y = az,color = 'AZ')) +
  scale_colour_manual(name="Axis",
                      values=c(AX="red", AY="blue", AZ="green")) +
  ylab("Raw ACC") + 
  xlab("Time") + 
  ggtitle("First 10,000 lines of accelerometer data")

# Save data
# save(accdata, file=paste(deploymentName,"-COMBINED", ".RData",sep=""))
# write.csv(accdata, file=paste(deploymentName,"-COMBINED", ".csv",sep=""), row.names = FALSE)
```

Step 1b: Extract and process GPS data
```{r}
if(depYear>2017){
# Extract GPS
gpsData <- accdata[,c("dttz","dt","Sats","HDOP","Lat","Long","FixAge","DateUTC","TimeUTC","DateAge","Altitude","Course","Speed")]
# Remove blank lines
gpsData <- gpsData[!is.na(gpsData["Sats"]),]
gpsData$Lat <- gpsData$Lat/1000000
gpsData$Long <- gpsData$Long/1000000
# Remove obvious bad hits
gpsData <- gpsData %>% 
  arrange(dttz) %>% 
  dplyr::filter(between(Lat,-90,90),
         Sats > 2,
         between(Long, -180,180),
         Lat != 0, Long != 0)
tryCatch({
  #filter by speed and angle
  gpsData <- gpsData %>% 
    dplyr::filter(sdafilter(Lat,Long,dt,rep(3,length(gpsData$Lat)), vmax = 30, ang = c(15, 25), distlim = c(2500, 5000))!= "removed")}, 
# warning = function(war) {
#   # warning handler picks up where error was generated
#   print(paste("WARNING:  ",war))},
error=function(err){
  print(paste("SDA Filter Error:  ",err,", using Distance-only Filter instead"))
}, finally = {
# filter by speed only (30 m/s)
  gpsData <- gpsData %>% 
  dplyr::filter(vmask(Lat,Long,dt, vmax = 30)!= "removed")
}) # END tryCatch

# Create proper datetime objects from GPS datetimes (convert GMT to local)
gpsData$dtGPS <- as.POSIXct(paste(gpsData$DateUTC, gpsData$TimeUTC), format="%m/%d/%Y %H:%M:%S", tz='GMT')
attr(gpsData$dtGPS, "tzone") # check that dt is in GMT
gpsData$dttzGPS <- gpsData$dtGPS # set dttz to dt
attr(gpsData$dttzGPS, "tzone") <- tzOffset # change the timezone to tzOffset
attr(gpsData$dttzGPS, "tzone") # check that dttz is in local time
str(gpsData)

# check to make sure data is reasonably located
plot(gpsData$Long,gpsData$Lat)

rownames(gpsData) <- c()
}

# Save as .RData file
# save(gpsData, file=paste(deploymentName,"-GPSData", ".RData",sep=""))
# Optional: save as a CSV file
# write.csv(gpsData, file=paste(deploymentName,"-GPSData", ".csv",sep=""), row.names = FALSE)
```

Step 1c: Confirm regular sampling rate across data (and remove or interpolate if necessary)
```{r}
# Define sampling rate. Confirm the sampling rate you define aligns with the programmed frequency.
resampleRate = 50

# Part A: Import data
# Note: If continuing from above, skip this import and jump to Part B
# Option 1 -- import R.Data file
# load("20190818_H38-COMBINED.RData")
# Option 2 -- import a CSV file
# filename <- file.choose()
# Load the Combined Data File
# accdata <- read_csv(filename)

# Part B: Assumes that data has already been processed as accData
# Select only the columns related to acc data (i.e., remove GPS fields) and create new dataframe
data <- accdata[, c("name","ts","temp","ax","ay","az","mx","my","mz","dt","dttz")]
# rm(accdata)
# show a table with the frequency of frequencies
data %>%
  # seconds since the beginning
  mutate(secs_since = as.numeric(dttz - min(dttz))) %>%
  # group into within-seconds blocks
  group_by(secs_since) %>%
  # frequency and period of sampling
  dplyr::mutate(freq = n()) %>%
  ungroup %>%
  {table(.$freq)} -> freqCount
# Count of number of occurrances of each freq
freqCount / as.numeric(names(freqCount))
# Percentage of total of each freq
format((freqCount / as.numeric(names(freqCount)))/sum((freqCount / as.numeric(names(freqCount)))),scientific=FALSE)

# Find the actual number of samples that will be interpolated
frequencies <- data.frame(freqCount)
frequencies$samplingRate <- as.numeric(names(freqCount))

frequencies <- frequencies %>% 
  mutate(samplesInterpolated = case_when(
    samplingRate < resampleRate ~ (resampleRate-samplingRate)*(Freq/samplingRate),
    TRUE ~ 0.0),
    totalSamples = case_when(
    samplingRate > resampleRate ~ (Freq - Freq/samplingRate),
    TRUE ~ Freq*1.0)) 

freqSum <- frequencies %>% 
  summarize(totalInterpoated = sum(samplesInterpolated),
            totalSamples = sum(totalSamples),
            # note: this is in % (i.e. has already been multiplied by 100)
            percentInterpolated = sum(samplesInterpolated)/sum(totalSamples)*100)
freqSum

# Create a dataframe with period and frequency 
data2 <- data %>%
  # seconds since the beginning
  dplyr::mutate(secs_since = as.numeric(dttz - min(dttz))) %>% 
  # Filter out first and last seconds because they're partial
  dplyr::filter(secs_since > 0,
         secs_since < max(secs_since)) %>% 
  # reset seconds since the beginning (could just subtract 1?)
  dplyr::mutate(secs_since = secs_since-1) %>%
  #mutate(secs_since = as.numeric(dttz - min(dttz))) %>%
  # group into within-seconds blocks
  dplyr::group_by(secs_since) %>%
  # frequency and period of sampling
  dplyr::mutate(freq = n(),
                period = 1 / resampleRate,
                # fraction of a second since beginning of second (i.e. 0-1)
                frac_sec = (row_number() - 1) / resampleRate,
                # seconds since beginning (e.g. 9.456)
                true_since = secs_since + frac_sec) %>%
  ungroup %>%
  # Remove any greater than resampleRate 
  dplyr::filter(frac_sec<=.98) %>%
  # true time down to fractional second (e.g. 2018-06-07 16:57:12.1234)
  dplyr::mutate(true_time = min(dttz) + true_since,
         tsDif = c(0, diff(ts)))
  
# show a table with the frequency of frequencies
data2$freq %>% table -> freqCount
freqCount / as.numeric(names(freqCount))

#create a dataframe with regular sampling
data3 <- data.frame(true_time = seq(from = min(data2$true_time),
                               to = max(data2$true_time),
                               by = 1 / resampleRate)) %>%
  merge(data2,all=TRUE) #Merge with data2 (fills unmatched with NA)

#fill name into newly created NA rows
data3$name <- data3$name[1]

data3 <- data3[, c("true_time", "name","ts","temp","ax","ay","az","mx","my","mz","freq","secs_since","true_since", "tsDif")]
colnames(data3)[1] <- c("dttz")

data3$temp <- na.fill(na.approx(data3$temp, data3$dttz, na.rm = FALSE),"extend")
data3$ax <- na.fill(na.approx(data3$ax, data3$dttz, na.rm = FALSE),"extend")
data3$ay <- na.fill(na.approx(data3$ay, data3$dttz, na.rm = FALSE),"extend")
data3$az <- na.fill(na.approx(data3$az, data3$dttz, na.rm = FALSE),"extend")
data3$mx <- na.fill(na.approx(data3$mx, data3$dttz, na.rm = FALSE),"extend")
data3$my <- na.fill(na.approx(data3$my, data3$dttz, na.rm = FALSE),"extend")
data3$mz <- na.fill(na.approx(data3$mz, data3$dttz, na.rm = FALSE),"extend")
data3$true_since <- na.fill(na.approx(data3$true_since, data3$dttz, na.rm = FALSE),"extend")

#simple plot
# library(ggplot2)
# ggplot() +
#   geom_line(data = data3, aes(x = dttz, y = ax,color = 'AX')) +
#   geom_line(data = data3, aes(x = dttz, y = ay,color = 'AY')) +
#   geom_line(data = data3, aes(x = dttz, y = az,color = 'AZ')) +
#   scale_colour_manual(name="Axis",
#                       values=c(AX="red", AY="blue", AZ="green")) +
#   ylab("Raw ACC") + 
#   xlab("Time") 

# check results
origP <- data2 %>%
  dplyr::slice(1e6:(1e6+200)) %>%
  ggplot(aes(x = true_time,
             y = ax)) +
  geom_line() +
  geom_point(size = 1) +
  labs(title = 'Original data')

time_rng <- range(data2$true_time[1e6:(1e6+200)])

rediscP <- data3 %>%
  dplyr::filter(between(dttz, time_rng[1], time_rng[2])) %>%
  ggplot(aes(x = dttz,
             y = ax)) +
  geom_line() +
  geom_point(size = 1) +
  labs(title = 'Rediscretized data')
ggarrange(origP,rediscP,nrow=2)

data <- data3

# Save as .RData file
# save(data3, file=paste(resampleRate,"HZ-",basename(filename),".RData",sep=""))

# Optional: save as a CSV file
# write_csv(data3, file.path(dirname(filename), paste(resampleRate,"HZ-",basename(filename),sep="")))


# Clean global environment
rm(accdata)
rm(data2)
rm(data3)
rm(freqSum)
rm(frequencies)
rm(origP)
rm(rediscP)
rm(freqCount)
rm(plotLength)
rm(resampleRate)
rm(time_rng)
```

### Step 3: Apply calibrations
```{r}
# Part A: If continuing from above, skip this import and jump to Part B
# Import rediscretized data
# Option 1 -- Import CSV
# filename <- file.choose()
# data <- read_csv(filename, 
#                  col_types = cols(
#                    ax = col_double(),
#                    ay = col_double(),
#                    az = col_double(),
#                    mx = col_double(),
#                    my = col_double(),
#                    mz = col_double(),
#                    secs_since = col_double(),
#                    temp = col_double()))
# depid <- basename(filename)
# depid <- strsplit(depid,'-')
# depid <- depid[[1]][2]

## Option 2 -- Import R.Data
# load("50Hz-20190818_H38.RData")

# Part B
depid <- strsplit(deploymentName,'_')
depid <- depid[[1]][3]

# Transform axes to North East Up sensor orientation used in Tag Tools 
# Acc	 [x -y z]
data$axT <- data$ax * (1.0)
data$ayT <- data$ay * (-1.0)
data$azT <- data$az * (1.0)
# Create a matrix with Acc data
At <- cbind(data$axT,data$ayT,data$azT)
# Check for NA
sum(is.na(At))
# If NAs, remove using linear approximation
if(sum(is.na(At)>0)){
  Atnarm <- At
  Atnarm <- na.approx(Atnarm, na.rm = FALSE)
} else {
  Atnarm <- At
}
# Check again for NAs
sum(is.na(Atnarm))

# Transform axes of Mag to orientation used in Tag Tools
data$mxT <- data$mx * 1.0
data$myT <- data$my * (-1.0)
data$mzT <- data$mz * 1.0

# Create a matrix with Mag data
Mt <- cbind(data$mxT,data$myT,data$mzT)
# Check for NA
sum(is.na(Mt))
# If NAs, remove using linear approximation
if(sum(is.na(Mt))>0){
  Mtnarm <- Mt
  Mtnarm <- na.approx(Mtnarm, na.rm = FALSE)
} else {
  Mtnarm <- Mt
}
# Check again for NAs
sum(is.na(Mtnarm))

# Import calibration file for logger
# Check AM field strength (if it doesn't look right, calibrate based on field collected data)
cal <- read_csv(file.choose())
AccCal <- list(poly = cbind(cal$AccPoly1,cal$AccPoly2), cross = cbind(cal$AccCross1,cal$AccCross2,cal$AccCross3))
MagCal <- list(poly = cbind(cal$MagPoly1,cal$MagPoly2), cross = cbind(cal$MagCross1,cal$MagCross2,cal$MagCross3))

AtCal <- apply_cal(Atnarm,cal = AccCal, T = NULL)
list <- list(A = AtCal[10000:180000,])
plott(list,50)

MtCal <- apply_cal(Mtnarm,cal = MagCal, T = NULL)
list <- list(M = MtCal[10000:300000,])
# plott(list,50, interactive = T)
plott(list,50)

## NOTE: This Section is optional, only use if spikes in Mag Data. This turns values outside of threshold into NA and then interpolates.
hiThresh <- 45 # Look at plotted magnetometer for bad hits and find a threshold
loThresh <- -45
MtCalThresh <- MtCal
MtCalThresh[MtCalThresh > hiThresh] = NA
MtCalThresh[MtCalThresh < loThresh] = NA
sum(is.na(MtCalThresh))
MtCalThresh <- na.approx(MtCalThresh, na.rm = FALSE)

list <- list(M = MtCalThresh[10000:300000,])
plott(list,50)
sum(is.na(MtCalThresh))
# If all looks Good, Save the Data back to MtCal
MtCal <- MtCalThresh

#Compute field intensity of acceleration and magnetometer data, and the inclination angle of the magnetic field. This is useful for
#checking the quality of a calibration, for detecting drift, and for validating the mapping of the sensor axes to the tag axes.
AMcheck <- check_AM(A=AtCal,M=MtCal,fs=50)
# If it's a good calibration the mean should be around 9.81 ms2- 
mean(AMcheck$fstr[,1])  # Checks acc field strength (column 1) # This could be higher than 10 because it's not static only. Dynamic is also included.
mean(AMcheck$fstr[,2]) # Checks mag field strength (column 2)
plot(AMcheck$fstr[100000:1000000,1])  # Plots acc field strength (column 1)

pStart <- 1
pEnd <- 5000
ggplot()+
 geom_line(aes(x=seq(1,pEnd-pStart+1),y=AMcheck$fstr[pStart:pEnd,1]))
ggplotly()

# Use tag data to self calibrate
# JAF: the game plan is to either decimate and then send decimated data into the calibration thing OR send a cropped subset of the data into the calibration. then we'll check the field strength of that and if it's a value closer to gravity, we will choose that one. might also try dividing by the constants that the sensor manufacturer recommended, then calibrating. 

accdata$ax1 <- (accdata$ax*.061)/1000
accdata$ay1 <- (accdata$ay*.061)/1000
accdata$az1 <- (accdata$az*.061)/1000
mag <- sqrt(accdata$ax1^2+accdata$ay1^2+accdata$az1^2)
plot(mag[1:10000])


# The idea of calibration is that it comes up with a scaling factor for each axis that works out so that the three of them summate to 1g (does this by looking at a bunch of iterations and decides what's the best combination of values that makes this work)
# JAF: wants to run decimate function to create 1Hz data to send into AMcheck to get the calibration. try this first.
# Next send decimated data into the spherical cal (use method cross)
# Two priority items: 1) recreate cal values for all tags using cross method (reload CSV, crop, and run) and try to use those; and 2) alternative is in situ calibration by decimating and figuring out scaling factor OR take subset

# event->acceleration.x = accelData.x * _accel_mg_lsb;
#  event->acceleration.x /= 1000;
#  event->acceleration.x *= SENSORS_GRAVITY_STANDARD;
# // Linear Acceleration: mg per LSB
#define LSM9DS0_ACCEL_MG_LSB_2G (0.061F)
#define LSM9DS0_ACCEL_MG_LSB_4G (0.122F)
#define LSM9DS0_ACCEL_MG_LSB_6G (0.183F)
#define LSM9DS0_ACCEL_MG_LSB_8G (0.244F)

# Save Calibrated Data Back To Dataframe
data2 <-cbind(data[,1:4],AtCal,MtCal,data[12:14])
# Rename Columns
colnames(data2) <- c("dttz","name","ts","temp","Ax","Ay","Az",
                     "Mx","My","Mz","secs_since","true_since","tsDiff")
names(data2)
data <- data2


# Save as .RData file
save(data, file=paste(depid,"-50Hz.RData",sep=""))

# Optional: save as a CSV file
write_csv(data, file.path(dirname(filename), paste(depid,"-50Hz.csv",sep="")))

# Clean global environment
rm(AccCal)
rm(At)
rm(AtCal)
rm(Atnarm)
rm(Atstruct)
rm(cal)
rm(data2)
rm(list)
rm(MagCal)
rm(Mt)
rm(MtCal)
rm(Mtnarm)
rm(Mtstruct)

```

### Step 4: Downsample data to 1-Hz
Creates datax (dataframe containing 1-Hz downsampled data). Preserves data (dataframe containing 50-Hz data).
```{r}
# Part A: If continuing from above, skip this import and jump to Part B
# Select file to import
filename <- file.choose()
# Import original 50-Hz data
data <- read_csv(filename, 
                 col_types = cols(
                   #dttz = col_datetime(),
                   #dt = col_datetime(),
                   temp = col_double(), # Only comment this out for 2017/2019 tags
                   Ax = col_double(),
                   Ay = col_double(),
                   Az = col_double(),
                   Mx = col_double(),
                   My = col_double(),
                   Mz = col_double(),
                   # freq = col_double(), # Comment this out for 2017/2019 tags
                   secs_since = col_double()))
# Create deployment ID
depid <- basename(filename)
depid <- strsplit(depid,'-')
depid <- depid[[1]][1]
depid

# Confirm correct dttz, since it has not been retaining time zone when writing to CSV 
attr(data$dttz, "tzone") # Check tz
attr(data$dttz, "tzone") <- tzOffset # Use this to change from UTC to proper local time (time will change)
data$dttz <- force_tz(data$dttz,tzone=tzOffset) # If time is correct and tz is wrong, force the tz (time will NOT change)

# Part B:
# Subset to only include time and acc data
data <- data[,c("dttz","true_since","Ax","Ay","Az")]

# Down sample data (decimate each vector separately)
df <- 50 # Set decimation factor df
fs <- 50 # Set original sampling rate

# For datetime select every nth value
dttz <- data$dttz
a <- dttz
dttz_down <- a[seq(1, length(a), df)]

# For true_since select every nth value
true_since <- data$true_since
a <- true_since
true_since_down <- a[seq(1, length(a), df)]

# Create individual vectors from acc fields
Ax <- data$Ax
Ay <- data$Ay
Az <- data$Az

# Convert vectors to numeric matrix
Ax_mat <- matrix(Ax,ncol=1)
Ay_mat <- matrix(Ay,ncol=1)
Az_mat <- matrix(Az,ncol=1)

# Use decimate function
n <- 12*5
Ax_down <- decimate(Ax_mat,5,n=n,ftype="fir")
Ay_down <- decimate(Ay_mat,5,n=n,ftype="fir")
Az_down <- decimate(Az_mat,5,n=n,ftype="fir")

n <-12*10
Ax_down <- decimate(Ax_down,10,n=n,ftype="fir")
Ay_down <- decimate(Ay_down,10,n=n,ftype="fir")
Az_down <- decimate(Az_down,10,n=n,ftype="fir")

# Combine down sampled data into one dataframe
data_down <- cbind.data.frame(dttz_down,true_since_down,Ax_down,Ay_down,Az_down)
data <- data_down

# Create Bird ID after downsampling
data$ID <- depid

# Change column names to more practical shorter names
colnames(data)[1:6] <- c("dttz","true_since","Ax","Ay","Az","ID")
# Reorder columns
data <- data[,c("ID","dttz","true_since","Ax","Ay","Az")]

# Very important to run this line to reset sampling rate to down sampled rate
fs = fs/df

# Clean global environment
rm(Ax_mat)
rm(Ay_mat)
rm(Az_mat)
rm(data_down)
rm(a)
rm(Ax)
rm(Ax_down)
rm(Ay)
rm(Ay_down)
rm(Az)
rm(Az_down)
rm(df)
rm(dttz)
rm(dttz_down)
rm(fs)
rm(n)
rm(true_since)
rm(true_since_down)

```

### Step 5: Calculate metrics (ODBA)
```{r}
# Calculate ODBA (ms-2)
# Calculate static (running mean)
windowSize=11 
data <- data %>% 
  arrange(ID,dttz) %>% 
  group_by(ID) %>% 
  mutate(static_Ax=roll_mean(Ax,windowSize,fill=NA),
         static_Ay=roll_mean(Ay,windowSize,fill=NA),
         static_Az=roll_mean(Az,windowSize,fill=NA)) %>%
  ungroup

# Replace the NA's at the beginning with the first good value
data <- data %>%
  arrange(ID,dttz) %>%
  group_by(ID) %>%
  mutate(naUp = ifelse(row_number()<=(windowSize-1)/2+1,1,0),
         naDown = ifelse(row_number() > n() - ((windowSize-1)/2+1),1,0)) %>%# View
  ungroup %>% 
  group_by(ID,naUp) %>% 
  mutate(static_Ax = ifelse(naUp == 1,last(static_Ax),static_Ax),
         static_Ay = ifelse(naUp == 1,last(static_Ay),static_Ay),
         static_Az = ifelse(naUp == 1,last(static_Az),static_Az)) %>% 
  ungroup %>% 
  group_by(ID,naDown) %>% 
  mutate(static_Ax = ifelse(naDown == 1,first(static_Ax),static_Ax),
         static_Ay = ifelse(naDown == 1,first(static_Ay),static_Ay),
         static_Az = ifelse(naDown == 1,first(static_Az),static_Az)) %>% 
  ungroup
data$naUp <- NULL
data$naDown <- NULL

# Calculate dynamic (raw-static) and ODBA (sum abs dynamic)
data <- data %>%
  arrange(ID,dttz) %>%
  group_by(ID) %>% 
  mutate(dyn_Ax=Ax-static_Ax,
         dyn_Ay=Ay-static_Ay,
         dyn_Az=Az-static_Az,
         ODBA=sqrt(abs(dyn_Ax)+abs(dyn_Ay)+abs(dyn_Az))) %>% 
  ungroup #%>% View
data$ODBA <- data$ODBA/9.81
data <- subset(data, select = -c(static_Ax,static_Ay,static_Az,dyn_Ax,dyn_Ay,dyn_Az))

# Check for NAs
sapply(data, function(x) sum(is.na(x)))

rm(windowSize)
```

### Step 6: Calculate sunrise and sunset 
This runs SunriseSunsetTimes.r
```{r}
# Create Spatial Points (Lat/Long should be representative of the deployment location)

# Manually set Lat/Long
# data$long <- -61.31
# data$lat <- -51.75

# Use GPS data extracted from TWLogger to set Lat/Long
data$long <- mean(gpsData$Long)
data$lat <- mean(gpsData$Lat)

# Check dttz 
attr(data$dttz, "tzone") # Check tz

# If tz needs fixed use one of two following options:
# Option 1
# attr(data$dttz, "tzone") <- tzOffset # Changes from UTC to proper local time (time will change)
# Option 2
# data$dttz <- force_tz(data$dttz,tzone=tzOffset) # If time is correct and tz is wrong, force the tz (time will NOT change)
# attr(data$dttz, "tzone") # Check tz

data$date <- as.Date(data$dttz, tz = tzOffset)
data$yr <- year(data$dttz)

# Run the sunrise sunset script
source('SunriseSunsetTimes.r')
# Check for NAs
sapply(data, function(x) sum(is.na(x)))
# Determine Season (calculated from 2018 solstices and equinoxes)
data$yday <- yday(data$dttz)
data$season <- rep('NA')
#data[which(data$yday > 79 & data$yday <= 173),]$season <- 'Fall'
data[which(data$yday > 173 & data$yday <= 265),]$season <- 'Winter'
#data[which(data$yday > 265 & data$yday < 355),]$season <- 'Spring'
# data[which(data$yday >= 355 | data$yday <= 79),]$season <- 'Summer'
data$season <- as.factor(data$season)
unique(data$season)
```

### Step 7: Add covariates
```{r}
# data$dawnT <- hour(data$dawn) + second(data$dawn)/60
# data$duskT <- hour(data$dusk) + second(data$dusk)/60
```

## Plot Data

### Plot GPS data
This creates a map to visualize movements.
```{r}
## Map
mapZoom <- 13 # Adjust this to fit all the data (20 is all the way zoomed in)
##JAF THis mapping code requires a googleKey. Maybe not good for distribution since not everyone will have one. THe leaflet below might do the trick
#plot the extent of this map (NOTE: must adjust zoom parameter)
myMap <-  get_googlemap(center = c(lon = mean(c(min(gpsData$Long), max(gpsData$Long))),  lat = mean(c(min(gpsData$Lat),max(gpsData$Lat)))),
                        zoom = mapZoom, 
                        size = c(640,400),
                        scale = 2,
                        extent="device",
                        maptype="hybrid", #"terrain"
                        style = c(feature = "all", element = "labels", visibility = "off"))

#plot map
ggmap(myMap)+
  geom_point(data=gpsData,aes(x=as.numeric(as.character(Long)),y=as.numeric(as.character(Lat))),alpha = 0.5, color= 'yellow') +
  scale_alpha(guide = 'none') + 
  theme(legend.position="right", 
        plot.title = element_text(hjust = 0.5)) + 
  ggtitle(deploymentName) + coord_fixed(1.3) + 
  labs(
    x = "Longitude",
    y = "Latitude"
  )
#save map output as .png
ggsave(sprintf("%s_Map.png",deploymentName), plot = last_plot(), device = "png",
       scale = 2, width = 7, height = 5, units = "in", dpi = 300, limitsize = F)
```

### Create a Leaflet Map
This creates an interactive map to visualize movements.
```{r}
# initiate the leaflet instance and store it to a variable
m = leaflet() %>%
  # Base groups
  addProviderTiles(providers$Stamen.Toner, group = "Toner (default)") %>%
  addTiles(group = "OSM") %>%
  addProviderTiles(providers$Esri.OceanBasemap, group = "Oceans") %>%
  addProviderTiles(providers$Stamen.TonerLite, group = "Toner Lite") %>% 
  addCircleMarkers( 
    lng = gpsData$Long,  
    lat = gpsData$Lat,
    popup = gpsData$dttz, 
    radius = 5, 
    stroke = FALSE, 
    fillOpacity = 0.75,
    clusterOptions = markerClusterOptions(),
    group = deploymentName
  )  %>%
  addPolylines(
    lng = gpsData$Long, 
    lat = gpsData$Lat,
    group = "lines"
  ) %>% 
  # Layers control
  addLayersControl(
    baseGroups = c("Toner (default)", "OSM", "Oceans", "Toner Lite"),
    overlayGroups = c(deploymentName,"lines"),
    options = layersControlOptions(collapsed = F),
    position = "topright"
  )
# we can "run"/compile the map, by running the printing it
m

saveWidget(m, file=paste0(deploymentName,".html"))

rm(m)

```

