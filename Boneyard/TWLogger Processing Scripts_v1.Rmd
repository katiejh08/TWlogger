---
title: "TWLogger Processing Scripts"
authors: "James Fahlbusch & Katie Harrington"
date: "August 18, 2019"
output: html_document
---

This processes one logger at a time. Must run entire script for each logger. Before beginning script, download deployment files into folder titled YYYYMMDD_[deploymentname] (e.g. 20190814_H38). 

### Setup environment

```{r setup, include=FALSE}
setwd("~/Projects/R/TWlogger")
tzOffset <- "Etc/GMT+3" # Falklands time in the winter

library(tidyverse)
library(zoo)
library(dplyr)
library(tagtools)
library(lubridate)
library(plotly)
library(RcppRoll)
library(car)
library(gridExtra)
library(signal)
library(ggpubr)
library(ggplot2)
library(ggmap)
library(maps)
library(mapdata)
```

### Define release and retrieval time for deployment
```{r}
# Specify the start and end time of deployment (NOTE: will be specific to each deployment, use local deployment time)
startTime <- as.POSIXct(strptime("2019-08-18 12:00:00",format="%Y-%m-%d %H:%M:%S"),tz=tzOffset)
# Always use logger retrieval time (regardless if retrieved before battery died)
endTime <- as.POSIXct(strptime("2019-08-19 14:15:00",format="%Y-%m-%d %H:%M:%S"),tz=tzOffset)
```

### Step 1a: Combine raw CSV files 
This script combines all deployment CSVs into a single CSV and saves.
KJH: need to update with script James emailed in Aug 2019
```{r}
# Choose the folder containing the multiple CSV files
# Select the first CSV of the group you want to combine
filename <- file.choose("Select the first CSV of the group")
pathChoice <- dirname(filename)
# This imports all CSV files in the directory chosen
filenames <- list.files(path = pathChoice, pattern = "*.csv", all.files = FALSE, full.names = FALSE, recursive = FALSE, ignore.case = TRUE)
import.list <- plyr::llply(paste(pathChoice,"/",filenames,sep = ""), read.csv)
accdata <- do.call("rbind", sapply(paste(pathChoice,"/",filenames,sep = ""), read.csv, simplify = FALSE))
rm(import.list)
# Create a name column containing the date and tag number
accdata$name <- row.names(accdata)
deploymentName <- strsplit(accdata$name[1],'/')
deploymentName <- deploymentName[[1]][(length(deploymentName[[1]]))-1] #pulls name of deployment from row names (n-1 element of split string)
accdata$name <- deploymentName
#check to see if worked
str(accdata)
rm(pathChoice)
rm(filenames)

# Reorder columns to prepare to change column names
accdata <- accdata[,c("name", "Date.MM.DD.YYYY.","Time.hh.mm.ss.","Timestamp.Ms.","Temp.Raw.",
                      "ACCELX","ACCELY","ACCELZ","MAGX","MAGY","MAGZ","Sats","HDOP","Latitude","Longitude","FixAge","DateUTC","TimeUTC","DateAge","Altitude","Course","Speed")]
# Change column names to more practical shorter names
colnames(accdata)[1:22] <- c("name","date","time","ts","temp","ax","ay","az","mx","my","mz","Sats","HDOP","Lat","Long","FixAge","DateUTC","TimeUTC","DateAge","Altitude","Course","Speed")
rownames(accdata) <- c()

# Create proper datetime objects (convert GMT to local time zone of deployment)
accdata$dt <- as.POSIXct(paste(accdata$date, accdata$time), format="%m/%d/%Y %H:%M:%S", tz='GMT')
attr(accdata$dt, "tzone") # check that dt is in GMT
accdata$dttz <- accdata$dt # set dttz to dt
attr(accdata$dttz, "tzone") <- tzOffset # change the timezone to tzOffset
attr(accdata$dttz, "tzone") # check that dttz is in local time
str(accdata)

# Remove unused columns
accdata <- subset(accdata, select = -c(date,time) )

# Run subset() function to extract data for the selected timerange
accdata <- subset(accdata, accdata$dttz >= startTime & accdata$dttz <= endTime)
str(accdata)

# Export the combined data for later use
write.csv(accdata, file=paste(deploymentName,"-COMBINED", ".csv",sep=""), row.names = FALSE)
```

### Step 1b: Extract and process GPS data
This creates and saves a separate file containing only GPS data.
```{r}
# Extract GPS
gpsData <- accdata[,c("Sats","HDOP","Lat","Long","FixAge","DateUTC","TimeUTC","DateAge","Altitude","Course","Speed")]
#remove blank lines
gpsData <- gpsData[!is.na(gpsData["Sats"]),]
gpsData$Lat <- gpsData$Lat/1000000
gpsData$Long <- gpsData$Long/1000000
#remove bad hits
gpsData <- gpsData[gpsData["Sats"]>2,] # Keep anything with 3 or more Sats
gpsData <- gpsData[gpsData["Lat"]<=90,]
gpsData <- gpsData[gpsData["Long"]<=180,]
## for southern hemisphere deployments
gpsData <- gpsData[gpsData["Lat"]<0,]
gpsData <- gpsData[gpsData["Long"]<0,]
# create proper datetime objects (convert GMT to local)
gpsData$dt <- as.POSIXct(paste(gpsData$DateUTC, gpsData$TimeUTC), format="%m/%d/%Y %H:%M:%S", tz='GMT')
attr(gpsData$dt, "tzone") # check that dt is in GMT
gpsData$dttz <- gpsData$dt # set dttz to dt
attr(gpsData$dttz, "tzone") <- tzOffset # change the timezone to tzOffset
attr(gpsData$dttz, "tzone") # check that dttz is in local time
str(gpsData)

# check to make sure data is reasonably located
plot(gpsData$Long,gpsData$Lat)
rownames(gpsData) <- c()

write.csv(gpsData, file=paste(deploymentName,"-GPSData", ".csv",sep=""), row.names = FALSE)
```

### Step 2: Confirm regular sampling rate across data (and remove or interpolate if necessary)
KJH: need to update with script James emailed in Aug 2019
```{r}
# Part A: If continuing from above, skip this import and jump to Part B
filename <- file.choose()
# Load the Combined Data File
accdata <- read_csv(filename)

# Part B
data <- accdata[, c("name","ts","temp","ax","ay","az","mx","my","mz","dt","dttz")]
# rm(accdata)
# show a table with the frequency of frequencies
data %>%
  # seconds since the beginning
  mutate(secs_since = as.numeric(dttz - min(dttz))) %>%
  # group into within-seconds blocks
  group_by(secs_since) %>%
  # frequency and period of sampling
  dplyr::mutate(freq = n()) %>%
  ungroup %>%
  {table(.$freq)} -> freqCount
# Count of number of occurrances of each freq
freqCount / as.numeric(names(freqCount))
# Percentage of total of each freq
format((freqCount / as.numeric(names(freqCount)))/sum((freqCount / as.numeric(names(freqCount)))),scientific=FALSE)
# JAF to do: Find the actual number of samples that will be interpolated

#Make sure the sampling rate you choose aligns with the frequency
resampleRate = 50

#makeRegularWGPS <- function(filenameA, resampleRate){
  # Create a dataframe with period and frequency 
  data2 <- data %>%
    # seconds since the beginning
    mutate(secs_since = as.numeric(dttz - min(dttz))) %>% 
    # Filter out first and last seconds because they're partial
    filter(secs_since > 0,
           secs_since < max(secs_since)) %>% View
    # reset seconds since the beginning (could just subtract 1?)
    mutate(secs_since = secs_since-1) %>%
    #mutate(secs_since = as.numeric(dttz - min(dttz))) %>%
    # group into within-seconds blocks
    group_by(secs_since) %>%
    # frequency and period of sampling
    dplyr::mutate(freq = n(),
                  period = 1 / resampleRate,
                  # fraction of a second since beginning of second (i.e. 0-1)
                  frac_sec = (row_number() - 1) / resampleRate,
                  # seconds since beginning (e.g. 9.456)
                  true_since = secs_since + frac_sec) %>%
    ungroup %>%
    # Remove any greater than resampleRate 
    filter(frac_sec<=.98) %>%
    # true time down to fractional second (e.g. 2018-06-07 16:57:12.1234)
    mutate(true_time = min(dttz) + true_since,
           tsDif = c(0, diff(ts)))
  
  # show a table with the frequency of frequencies
  data2$freq %>% table -> freqCount
  freqCount / as.numeric(names(freqCount))
  
  #create a dataframe with regular sampling
  data3 <- data.frame(true_time = seq(from = min(data2$true_time),
                                 to = max(data2$true_time),
                                 by = 1 / resampleRate)) %>%
    merge(data2,all=TRUE) #Merge with data2 (fills unmatched with NA)
  
  #fill name into Newly created NA rows
  data3$name <- data3$name[1]
  
  data3 <- data3[, c("true_time", "name","ts","temp","ax","ay","az","mx","my","mz","freq","secs_since","true_since", "tsDif")]
  colnames(data3)[1] <- c("dttz")
  
  data3$temp <- na.fill(na.approx(data3$temp, data3$dttz, na.rm = FALSE),"extend")
  data3$ax <- na.fill(na.approx(data3$ax, data3$dttz, na.rm = FALSE),"extend")
  data3$ay <- na.fill(na.approx(data3$ay, data3$dttz, na.rm = FALSE),"extend")
  data3$az <- na.fill(na.approx(data3$az, data3$dttz, na.rm = FALSE),"extend")
  data3$mx <- na.fill(na.approx(data3$mx, data3$dttz, na.rm = FALSE),"extend")
  data3$my <- na.fill(na.approx(data3$my, data3$dttz, na.rm = FALSE),"extend")
  data3$mz <- na.fill(na.approx(data3$mz, data3$dttz, na.rm = FALSE),"extend")
  data3$true_since <- na.fill(na.approx(data3$true_since, data3$dttz, na.rm = FALSE),"extend")
  
  #simple plot
  # library(ggplot2)
  # ggplot() +
  #   geom_line(data = data3, aes(x = dttz, y = ax,color = 'AX')) +
  #   geom_line(data = data3, aes(x = dttz, y = ay,color = 'AY')) +
  #   geom_line(data = data3, aes(x = dttz, y = az,color = 'AZ')) +
  #   scale_colour_manual(name="Axis",
  #                       values=c(AX="red", AY="blue", AZ="green")) +
  #   ylab("Raw ACC") + 
  #   xlab("Time") 
  
  # check results
  x11()
  data2 %>%
    slice(1e6:(1e6+100)) %>%
    ggplot(aes(x = true_time,
               y = mx)) +
    geom_line() +
    geom_point(size = 1) +
    labs(title = 'Original data')

  time_rng <- range(data2$true_time[1e6:(1e6+100)])
  x11()
  data3 %>%
    filter(between(dttz, time_rng[1], time_rng[2])) %>%
    ggplot(aes(x = dttz,
               y = mx)) +
    geom_line() +
    geom_point(size = 1) +
    labs(title = 'Rediscretized data')
  
  write_csv(data3, file.path(dirname(filename), paste(resampleRate,"HZ-",basename(filename),sep="")))
#} 
```

### Step 3: Apply calibrations
```{r}
# Part A: If continuing from above, skip this import and jump to Part B
# Import rediscretized data
filename <- file.choose()

data <- read_csv(filename, 
                 col_types = cols(
                   ax = col_double(),
                   ay = col_double(),
                   az = col_double(),
                   mx = col_double(),
                   my = col_double(),
                   mz = col_double(),
                   secs_since = col_double(),
                   temp = col_double()))
depid <- basename(filename)
depid <- strsplit(depid,'-')
depid <- depid[[1]][2]

# Part B
depid <- strsplit(deploymentName,'_')
depid <- depid[[1]][2]

# Transform axes to North East Up sensor orientation used in Tag Tools 
# Acc	 [x -y z]
data$axT <- data$ax * (1.0)
data$ayT <- data$ay * (-1.0)
data$azT <- data$az * (1.0)
# Create a matrix with Acc data
At <- cbind(data$axT,data$ayT,data$azT)
# Check for NA
#count(is.na(At))
sum(is.na(At))
# If NAs, remove using linear approximation
if(sum(is.na(At)>0)){
  Atnarm <- At
  Atnarm <- na.approx(Atnarm, na.rm = FALSE)
} else {
  Atnarm <- At
}
# Check again for NAs
sum(is.na(Atnarm))
# Create an Acc sensor structure using At (NOTE: manually change fs value if not 50)
Atstruct <- sens_struct(data = Atnarm, fs = 50, depid = depid,type='acc')
# Plot 
# Alist <- list(A = Atstruct$data)
# plott(Alist,50)

data$mxT <- data$mx * 1.0
data$myT <- data$my * (-1.0)
data$mzT <- data$mz * 1.0
# Create a matrix with Mag data
Mt <- cbind(data$mxT,data$myT,data$mzT)
# Check for NA
sum(is.na(Mt))
# If NAs, remove using linear approximation
if(sum(is.na(Mt))>0){
  Mtnarm <- Mt
  Mtnarm <- na.approx(Mtnarm, na.rm = FALSE)
} else {
  Mtnarm <- Mt
}
# Check again for NAs
sum(is.na(Mtnarm))
# Create an Mag sensor structure using Mt (NOTE: manually change fs value if not 50)
Mtstruct <- sens_struct(data = Mtnarm, fs = 50, depid = depid,type='mag')
# Plot 
# Mlist <- list(M = Mtstruct$data)
# plott(Mlist,50)
# import calibration file for logger
cal <- read_csv(file.choose())
AccCal <- list(poly = cbind(cal$AccPoly1,cal$AccPoly2), cross = cbind(cal$AccCross1,cal$AccCross2,cal$AccCross3))
MagCal <- list(poly = cbind(cal$MagPoly1,cal$MagPoly2), cross = cbind(cal$MagCross1,cal$MagCross2,cal$MagCross3))

AtCal <- apply_cal(Atnarm,cal = AccCal, T = NULL)
list <- list(A = AtCal[10000:180000,])
plott(list,50)
MtCal <- apply_cal(Mtnarm,cal = MagCal, T = NULL)
list <- list(M = MtCal[10000:300000,])
# plott(list,50, interactive = T)
plott(list,50)

    ## NOTE: This Section is optional, only use if spikes in Mag Data.
    # Look at plotted magnetometer for bad hits and find a threshold
    hiThresh <- 45
    loThresh <- -45
    MtCalThresh <- MtCal
    MtCalThresh[MtCalThresh > hiThresh] = NA
    MtCalThresh[MtCalThresh < loThresh] = NA
    sum(is.na(MtCalThresh))
    MtCalThresh <- na.approx(MtCalThresh, na.rm = FALSE)
    
    list <- list(M = MtCalThresh[10000:300000,])
    plott(list,50)
    sum(is.na(MtCalThresh))
    # If all looks Good, Save the Data back to MtCal
    MtCal <- MtCalThresh

#Save Calibrated Data Back To Dataframe
data2 <-cbind(data[,1:4],AtCal,MtCal,data[12:14])
#Rename Columns
colnames(data2) <- c("dttz","name","ts","temp","Ax","Ay","Az",
                     "Mx","My","Mz","secs_since","true_since","tsDiff")
names(data2)

#Save Output
write_csv(data2, file.path(dirname(filename), paste(depid,"-50Hz.csv",sep="")))
```

### Step 4: Downsample data to 1-Hz
Creates datax (dataframe containing 1-Hz downsampled data). Preserves data (dataframe containing 50-Hz data).
```{r}
# Part A: If continuing from above, skip this import and jump to Part B
# Select file to import
filename <- file.choose()
# Import original 50-Hz data
data <- read_csv(filename, 
                 col_types = cols(
                   #dttz = col_datetime(),
                   #dt = col_datetime(),
                   temp = col_double(), # Only comment this out for 2017/2019 tags
                   Ax = col_double(),
                   Ay = col_double(),
                   Az = col_double(),
                   Mx = col_double(),
                   My = col_double(),
                   Mz = col_double(),
                   # freq = col_double(), # Comment this out for 2017/2019 tags
                   secs_since = col_double()))
# Create deployment ID
depid <- basename(filename)
depid <- strsplit(depid,'-')
depid <- depid[[1]][1]
depid

# Confirm correct dttz, since it has not been retaining time zone when writing to CSV 
attr(data$dttz, "tzone") # Check tz
attr(data$dttz, "tzone") <- tzOffset # Use this to change from UTC to proper local time (time will change)
data$dttz <- force_tz(data$dttz,tzone=tzOffset) # If time is correct and tz is wrong, force the tz (time will NOT change)

# Part B:
# Subset to only include time and acc data
data <- data[,c("dttz","true_since","Ax","Ay","Az")]

# Down sample data (decimate each vector separately)
df <- 50 # Set decimation factor df
fs <- 50 # Set original sampling rate

# For datetime select every nth value
dttz <- data$dttz
a <- dttz
dttz_down <- a[seq(1, length(a), df)]

# For true_since select every nth value
true_since <- data$true_since
a <- true_since
true_since_down <- a[seq(1, length(a), df)]

# Create individual vectors from acc fields
Ax <- data$Ax
Ay <- data$Ay
Az <- data$Az

# Convert vectors to numeric matrix
Ax_mat <- matrix(Ax,ncol=1)
Ay_mat <- matrix(Ay,ncol=1)
Az_mat <- matrix(Az,ncol=1)

# Use decimate function
n <- 12*5
Ax_down <- decimate(Ax_mat,5,n=n,ftype="fir")
Ay_down <- decimate(Ay_mat,5,n=n,ftype="fir")
Az_down <- decimate(Az_mat,5,n=n,ftype="fir")

n <-12*10
Ax_down <- decimate(Ax_down,10,n=n,ftype="fir")
Ay_down <- decimate(Ay_down,10,n=n,ftype="fir")
Az_down <- decimate(Az_down,10,n=n,ftype="fir")

# Combine down sampled data into one dataframe
data_down <- cbind.data.frame(dttz_down,true_since_down,Ax_down,Ay_down,Az_down)
data <- data_down

# Create Bird ID after downsampling
data$ID <- depid

# Change column names to more practical shorter names
colnames(data)[1:6] <- c("dttz","true_since","Ax","Ay","Az","ID")
# Reorder columns
data <- data[,c("ID","dttz","true_since","Ax","Ay","Az")]

# Very important to run this line to reset sampling rate to down sampled rate
fs = fs/df
```

### Step 5: Calculate metrics (ODBA)
```{r}
# Calculate ODBA (ms-2)
# Calculate static (running mean)
windowSize=11 
data <- data %>% 
  arrange(ID,dttz) %>% 
  group_by(ID) %>% 
  mutate(static_Ax=roll_mean(Ax,windowSize,fill=NA),
         static_Ay=roll_mean(Ay,windowSize,fill=NA),
         static_Az=roll_mean(Az,windowSize,fill=NA)) %>%
  ungroup

# Replace the NA's at the beginning with the first good value
data <- data %>%
  arrange(ID,dttz) %>%
  group_by(ID) %>%
  mutate(naUp = ifelse(row_number()<=(windowSize-1)/2+1,1,0),
         naDown = ifelse(row_number() > n() - ((windowSize-1)/2+1),1,0)) %>%# View
  ungroup %>% 
  group_by(ID,naUp) %>% 
  mutate(static_Ax = ifelse(naUp == 1,last(static_Ax),static_Ax),
         static_Ay = ifelse(naUp == 1,last(static_Ay),static_Ay),
         static_Az = ifelse(naUp == 1,last(static_Az),static_Az)) %>% 
  ungroup %>% 
  group_by(ID,naDown) %>% 
  mutate(static_Ax = ifelse(naDown == 1,first(static_Ax),static_Ax),
         static_Ay = ifelse(naDown == 1,first(static_Ay),static_Ay),
         static_Az = ifelse(naDown == 1,first(static_Az),static_Az)) %>% 
  ungroup
data$naUp <- NULL
data$naDown <- NULL

# Calculate dynamic (raw-static) and ODBA (sum abs dynamic)
data <- data %>%
  arrange(ID,dttz) %>%
  group_by(ID) %>% 
  mutate(dyn_Ax=Ax-static_Ax,
         dyn_Ay=Ay-static_Ay,
         dyn_Az=Az-static_Az,
         ODBA=abs(dyn_Ax+dyn_Ay+dyn_Az)) %>% 
  ungroup #%>% View
data$ODBA <- data$ODBA
data <- subset(data, select = -c(static_Ax,static_Ay,static_Az,dyn_Ax,dyn_Ay,dyn_Az))

# Check for NAs
sapply(data, function(x) sum(is.na(x)))
```

### Step 6: Calculate sunrise and sunset 
This runs SunriseSunsetTimes.r
```{r}
# Create Spatial Points (Lat/Long should be representative of the deployment location)
data$long <- -61.31
data$lat <- -51.75

# Check dttz 
attr(data$dttz, "tzone") # Check tz
# If tz needs fixed use one of two following options:
# Option 1
# attr(data$dttz, "tzone") <- tzOffset # Changes from UTC to proper local time (time will change)
# Option 2
# data$dttz <- force_tz(data$dttz,tzone=tzOffset) # If time is correct and tz is wrong, force the tz (time will NOT change)
attr(data$dttz, "tzone") # Check tz

data$date <- as.Date(data$dttz, tz = tzOffset)
data$yr <- year(data$dttz)

# Run the sunrise sunset script
source('SunriseSunsetTimes.r')
# Check for NAs
sapply(data, function(x) sum(is.na(x)))
# Determine Season (calculated from 2018 solstices and equinoxes)
data$yday <- yday(data$dttz)
data$season <- rep('NA')
#data[which(data$yday > 79 & data$yday <= 173),]$season <- 'Fall'
data[which(data$yday > 173 & data$yday <= 265),]$season <- 'Winter'
#data[which(data$yday > 265 & data$yday < 355),]$season <- 'Spring'
data[which(data$yday >= 355 | data$yday <= 79),]$season <- 'Summer'
data$season <- as.factor(data$season)
unique(data$season)
```

### Step 7: Add covariates
```{r}
# data$dawnT <- hour(data$dawn) + second(data$dawn)/60
# data$duskT <- hour(data$dusk) + second(data$dusk)/60
```

## Plot Data

### Plot GPS data
This creates a map to visualize movements.
```{r}
## Map
mapZoom <- 13 # KJH!! Adjust this to fit all the data (20 is all the way zoomed in)
#plot the extent of this map (NOTE: must adjust zoom parameter)
myMap <-  get_googlemap(center = c(lon = mean(c(min(gpsData$Long), max(gpsData$Long))),  lat = mean(c(min(gpsData$Lat),max(gpsData$Lat)))),
                        zoom = mapZoom, 
                        size = c(640,400),
                        scale = 2,
                        extent="device",
                        maptype="hybrid", #"terrain"
                        style = c(feature = "all", element = "labels", visibility = "off"))

#plot map
ggmap(myMap)+
  geom_point(data=gpsData,aes(x=as.numeric(as.character(Long)),y=as.numeric(as.character(Lat))),alpha = 0.5, color= 'yellow') +
  scale_alpha(guide = 'none') + 
  theme(legend.position="right", 
        plot.title = element_text(hjust = 0.5)) + 
  ggtitle(deploymentName) + coord_fixed(1.3) + 
  labs(
    x = "Longitude",
    y = "Latitude"
  )
#save map output as .png
ggsave(sprintf("%s_Map.png",deploymentName), plot = last_plot(), device = "png",
       scale = 2, width = 7, height = 5, units = "in", dpi = 300, limitsize = F)
```
